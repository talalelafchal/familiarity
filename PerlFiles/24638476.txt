Why is Parallel::ForkManager creating sub-processes fine, but not processing them in parallel
<p>Working through some legacy code am I'm stumped on the following issue.</p> <p>We have a program that automates the pushing of configurations out to multiple routers. The Perl program that does this uses the <code>Parallel::ForkManager</code> library to accomplish parallelization of these pushes. </p> <p>Early on in the program the following occurs...</p> <pre><code>$p = new Parallel::ForkManager($multi); </code></pre> <p>The main loop that creates the forks is as follows...</p> <pre><code>foreach my $node (@files) { # only if we're running in parallel if (ref($p)) { $p-&gt;start() and next; } # ship the file &amp; path off to the main sub that does # all the heavy lifting do_push($node); # only if we're running in parallel if (ref($p)) { $p-&gt;finish(); } } # only if we're running in parallel if (ref($p)) { $p-&gt;wait_all_children(); } </code></pre> <p>The <code>do_push($node)</code> subroutine is too long to detail here, but in short, it establishes a logging file for the node, then connects via ssh to the router and pushes the configurations. It then exits and finalizes the logging. </p> <p>From the surface, everything appears to be working, the processes fork, and the results are as expected. </p> <p>However...</p> <p>While separate processes are created, the connection of the routers, and the pushing of the configurations is not happening in parallel. It was easily verified via <code>netstat -an</code> that the ssh connections were only occurring in sequence, rather than in parallel. Only after one connection closed out, would the next router connect. In short, I'm only getting one ssh connection at a time.</p> <p>Does anyone have any idea why this would be, or where to proceed to resolve this?</p> <p>[edit] </p> <p>Based on comments, here is a summary of what is happening in the subroutine. I can't put it all in here, both for space and security reasons.</p> <p>The start of the subroutine is as follows...</p> <pre><code>sub do_push { my($data) = @_; my($path,$router) = @$data; $|++; # hopefully speed things up ($DEBUG or ($v &gt; 2)) &amp;&amp; print STDERR "PROCESSING: $router ($path)\n"; </code></pre> <p>The process that follows is...</p> <ul> <li>Open a logging file, write to it.</li> <li>Instantiate the module(s) that do the connection to the router.</li> <li>Connect to the router.</li> <li>Push commands, verify commands, commit if successful.</li> <li>Log all the while to the logging file.</li> </ul> <p>The end of the subroutine is as follows...</p> <pre><code> # finished with this router... # append our archive file my $ts = tv_interval($t0, [gettimeofday()]); (!$DEBUG &amp;&amp; $archive) &amp;&amp; print S "ROUTER: $router:$ts:$disposition\n"; ($DEBUG or ($v &gt; 2)) &amp;&amp; print STDERR "ROUTER COMPLETE: $router -&gt; $ts sec\n"; # unlock and close our files (($DEBUG &gt; 1) or ($v &gt; 2)) &amp;&amp; print STDERR "UNLOCK AND CLOSE: $archfile\n"; (($DEBUG &gt; 1) or ($v &gt; 2)) &amp;&amp; print STDERR "UNLOCK AND CLOSE: $summary\n"; (!$DEBUG &amp;&amp; $archive) &amp;&amp; flock(F, LOCK_UN); (!$DEBUG &amp;&amp; $archive) &amp;&amp; flock(S, LOCK_UN); (!$DEBUG &amp;&amp; $archive) &amp;&amp; close(F); (!$DEBUG &amp;&amp; $archive) &amp;&amp; close(S); return; } </code></pre> <p>If run in debug mode the first debug statement in the subroutine: <code>"PROCESSING: $router ($path)\n"</code> only prints after the previous router has <strong>fully completed the subroutine</strong>. There definitely <strong>are multiple processes forking, I've verified that</strong>. It's just that "something" is blocking the process from running until the previous process completes. I'm struggling with finding what that "something" is. </p> <p>Any help is appreciated.</p>
<p><strong>[solved]</strong></p> <p>I hate answering my own questions, however I did find the reason for the block...</p> <p>It turned out that the problem was a file access problem. In the <code>do_push()</code> loop the program was logging to two files. One file was to log the results of the file itself, this file was unique to the sub-process, no problem. However the program was also writing to a "summary" file. This file was being logged to by all the sub-processes.</p> <p>What would happen is that the first thread would open the file, log some stuff, connect to the router and push out commands. Once it was done, it would close the file. Of course the other threads would block, waiting for access to the "summary" file. </p> <p>For small router pushes, it all happened so fast that it appeared that this was all happening in parallel. It was only when we had to push out 1000's of lines to each router did it become apparent that this wasn't going on in parallel. </p>