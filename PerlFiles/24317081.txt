Speeding up a search through an array in Perl by altering data structure
<p>I have a subroutine, which receives a word stored as a string. This string is then compared to every line of an array containing many lines. Each line contains one or more words. The point is to compare the incoming word to each line of the array to see if it matches ANY of the words in the lines of the array, first listed or otherwise. </p> <p>The subroutine receives a string value called <code>$name</code>. <code>$name</code> is then used to search through a text file that is stored in an array called <code>@ASlist</code>. <code>foreach</code> line in <code>@ASlist</code>, the line is split into cells and made into another array called <code>@synonymns</code>. This is cycled through using a <code>for</code> loop, in which a matching operator is working to find if <code>$name</code> matches with <code>$synonymns[$i]</code>. When a match is found, the first and matching entry in <code>@synonyms</code> goes into a container <code>@collectmatches</code>.</p> <p>For example, if <code>$name</code> is <code>hot</code>, it would need to match up with the following line which could be anywhere in the array</p> <pre><code>luke, warm, hot, burning </code></pre> <p>and return the values <code>(luke, hot)</code>.</p> <pre><code>sub compareAS { my ($name) = @_; chomp $name; my @collectmatches = (); foreach my $ASline ( @ASlist ){ my @synonyms = split("\t", $ASline ); for ( my $i = 0; $i &lt; scalar @synonyms; $i++ ){ chomp $synonyms[ $i ]; if ( $name =~m/\b\Q$synonyms[$i]\E\b/i ){ push ( @collectmatches, $synonyms[0], $synonyms[$i] ); } else { } } } return @collectmatches; } </code></pre> <p>When the source of my <code>$names</code> is big, this is very slow, taking overnight to run. A hash does not seem appropriate, for two reasons. One, a hash is a one-to-one type of mapping, and I have lists of synonyms where the <code>$name</code> can be potentially matched to one of many. Two, while the keys of a hash can be quickly searched using <code>exist</code>, the values cannot. Knowing if a value existed would speed things up. Is there some other way I could structure my data to speed this process?</p> <p>One idea is to also have the array stored as a giant string, and see if <code>$name</code> can be matched somewhere within the array. If not, the sub can return nothing, and the script can go on to send in another <code>$name</code> to the sub. If so, the match might be spurious or real. If a match is made, then the array can be search line by line, as above. If it doesn't find a match, the match was spurious, and the script can go on with a new <code>$name</code>. If it does find a match, it gets collected into <code>@collectmatches</code>. </p>
<p>Just use a hash, like you already proposed:</p> <pre><code>my %synonyms; BEGIN { for (@ASlist) { my ($keyword) = my @words = split ' '; push @{$synonyms{lc $_}}, $keyword for @words; } } sub compareAS { my ($name) = @_; my @collectmatches = (); for my $word (split ' ', $name) { for my $keyword (@{$synonyms{lc $word}}) { push @collectmatches, $keyword, $word; } } return @collectmatches; } </code></pre>
<p>Your requirement is very unclear, but I suggest that your <code>@ASlist</code> could be replaced by a hash, where the keys are all the words anywhere in the lines, and the value is a reference to an array of all the lines that it appears in.</p> <p>This program shows the idea. If you gave some more representative data then it could better demonstrate the principle. In this case, each line from the file is converted into an array of words</p> <pre><code>[ 'luke', 'warm', 'hot', 'burning' ] </code></pre> <p>and the hash has four keys <code>luke</code>, <code>warm</code>, <code>hot</code>, and <code>burning</code>. The values corresponding to each of these keys is a list of all the lists in which that word appears, so after reading in the file, the hash <code>%ASlist</code> looks like this</p> <pre><code>( burning =&gt; [["luke", "warm", "hot", "burning"]], hot =&gt; [["luke", "warm", "hot", "burning"]], luke =&gt; [["luke", "warm", "hot", "burning"]], warm =&gt; [["luke", "warm", "hot", "burning"]], ) </code></pre> <p>I have kept to the output that you say you want, being the first word in each line and the word that was being searched for, but this seems a little strange and I imagine is a convenience for some other purpose that you don't mention. The <code>map</code> call at the end of the subroutine is there to discard all but those two words.</p> <p>It would be simpler if a word could only ever appear only in one line in the file, but you haven't said anything about that so I have assumed that a word can appear many times in many lines. Hence the result of the subroutine is an array of arrays.</p> <pre><code>use strict; use warnings; use autodie; open my $as_fh, '&lt;', 'as.txt'; my %ASlist; while (&lt;$as_fh&gt;) { chomp; my @words = split /\t/; push @{ $ASlist{$_} }, \@words for @words; } use Data::Dump; dd synonyms('hot'); sub synonyms { my ($name) = @_; return unless my $match = $ASlist{$name}; map [ $_-&gt;[0], $name ], @$match; } </code></pre> <p><strong>output</strong></p> <pre><code>["luke", "hot"] </code></pre>
<p>I've done this before on a file with 500,000 lines. I found Perl grep to be fast enough for me. The reason: customer file had a column that contained a key value, and this key value was repeated on multiple rows. I had to find every row that contained a key value. </p> <pre><code>my($i,$findme,@file,@found,$fn); $findme="1234-abc"; # Find this value on every line in the file. $fn="input.txt"; open(INFILE,"&gt;".$fn) || die "ERROR: Could not open $fn"; @file=&lt;INFILE&gt;; # Read whole file into array @found=grep(/$findme/g,@file); # Each entry in @found contains the whole line that had a match with $findme for each $i (@found) {print "Found $i\n"; } close(INFILE); </code></pre>