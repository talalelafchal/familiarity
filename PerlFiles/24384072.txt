print errors in log and continue to crawl other url
<p>Hi I wish to process array of urls .If there is an issue with one url that has to recorded in errorfile.html and continue to process other urls.(either url failed to load or xpath failed error) has to be recored in errorlog. getting an error "cant call method "isa" on an unidentified "</p> <pre><code> use LWP::Simple; use File::Compare; use HTML::TreeBuilder::XPath; use LWP::UserAgent; { open(FILE, "C:/Users/jeyakuma/Desktop/shipping project/input/input.txt"); { while(&lt;FILE&gt;) { chomp; $url=$_; foreach ($url) { ($domain) = $url =~ m|www.([A-Z a-z 0-9]+.{3}).|x; } do 'C:/Users/jeyakuma/Desktop/perl/mainsub.pl'; &amp;domain_check(); my $ua = LWP::UserAgent-&gt;new( agent =&gt; "Mozilla/5.0" ); my $req = HTTP::Request-&gt;new( GET =&gt; "$url" ); my $res = $ua-&gt;request($req); if ( $res-&gt;is_success ) { print "working on $competitor\n"; binmode ":utf8"; my $xp = HTML::TreeBuilder::XPath-&gt;new_from_url($url); print "Extracting the $competitor xpath\n"; my @node = $xp-&gt;findnodes_as_string("$xpath") or print "couldn't find the node\n"; open HTML, '&gt;:encoding(cp1252)',"C:/Users/jeyakuma/Desktop/die/$competitor.html"; foreach(&lt;@node&gt;) { print HTML @node; close HTML ; } } else{ print "In valid url"; } } } } </code></pre>
<pre><code>I wish to process array of urls </code></pre> <p>Then modify your script to use looping on array.</p> <p>Something like</p> <pre><code>foreach my $url (@URLS){ #work on $url here my $xp = HTML::TreeBuilder::XPath-&gt;new_from_url($url); my @node = $xp-&gt;findnodes_as_strings('//div[@class="mainbox-body"]'); #don't use die, instead record error message in file. print $error_log "node doesn't exist" unless @node; #do other tasks for url } </code></pre> <p><strong>Edit:</strong> Use the below piece of code, it worked fine for me. Also what is <code>$xpath</code> in your script? That is the part which is giving you <code>isa</code> error (which you mentioned in comment)</p> <pre><code>#!/usr/bin/perl use strict; use warnings; use LWP::Simple; use HTML::TreeBuilder::XPath; use LWP::UserAgent; #You can read URLS from file and create array, I'm doing directly for simplicity my @urls = ("http://www.google.com", "http://www.yahoo.com"); foreach my $url (@urls){ print "working on $url\n"; my $ua = LWP::UserAgent-&gt;new( agent =&gt; "Mozilla/5.0" ); my $req = HTTP::Request-&gt;new( GET =&gt; "$url" ); my $res = $ua-&gt;request($req); if ( $res-&gt;is_success ) { print "In if block, success\n"; my $xp = HTML::TreeBuilder::XPath-&gt;new_from_url($url); my $node = $xp-&gt;findnodes_as_string('//div[@class="mainbox-body"]') or print "couldn't find the node\n"; } else{ print "In else block\n"; } } </code></pre>