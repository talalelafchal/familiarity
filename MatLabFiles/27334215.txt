Neural Network not fitting XOR
<p>I created an Octave script for training a neural network with 1 hidden layer using backpropagation but it can not seem to fit an XOR function.</p> <ul> <li><code>x</code> Input 4x2 matrix <code>[0 0; 0 1; 1 0; 1 1]</code></li> <li><code>y</code> Output 4x1 matrix <code>[0; 1; 1; 0]</code></li> <li><code>theta</code> Hidden / output layer weights</li> <li><code>z</code> Weighted sums</li> <li><code>a</code> Activation function applied to weighted sums</li> <li><code>m</code> Sample count (<code>4</code> here)</li> </ul> <p>My weights are initialized as follows</p> <pre><code>epsilon_init = 0.12; theta1 = rand(hiddenCount, inputCount + 1) * 2 * epsilon_init * epsilon_init; theta2 = rand(outputCount, hiddenCount + 1) * 2 * epsilon_init * epsilon_init; </code></pre> <p>Feed forward</p> <pre><code>a1 = x; a1_with_bias = [ones(m, 1) a1]; z2 = a1_with_bias * theta1'; a2 = sigmoid(z2); a2_with_bias = [ones(size(a2, 1), 1) a2]; z3 = a2_with_bias * theta2'; a3 = sigmoid(z3); </code></pre> <p>Then I compute the logistic cost function</p> <pre><code>j = -sum((y .* log(a3) + (1 - y) .* log(1 - a3))(:)) / m; </code></pre> <p>Back propagation</p> <pre><code>delta2 = (a3 - y); gradient2 = delta2' * a2_with_bias / m; delta1 = (delta2 * theta2(:, 2:end)) .* sigmoidGradient(z2); gradient1 = delta1' * a1_with_bias / m; </code></pre> <p>The gradients were verified to be correct using gradient checking.</p> <p>I then use these gradients to find the optimal values for theta using gradient descent, though using Octave's <code>fminunc</code> function yields the same results. The cost function converges to <code>ln(2)</code> (or <code>0.5</code> for a squared errors cost function) because the network outputs <code>0.5</code> for all four inputs no matter how many hidden units I use.</p> <p>Does anyone know where my mistake is?</p>
<p>Start with a larger range when initialising weights, including negative values. It is difficult for your code to "cross-over" between positive and negative weights, and you probably meant to put <code>* 2 * epsilon_init - epsilon_init;</code> when instead you put <code>* 2 * epsilon_init * epsilon_init;</code>. Fixing that may well fix your code.</p> <p>As a rule of thumb, I would do something like this:</p> <pre><code>theta1 = ( 0.5 * sqrt ( 6 / ( inputCount + hiddenCount) ) * randn( hiddenCount, inputCount + 1 ) ); theta2 = ( 0.5 * sqrt ( 6 / ( hiddenCount + outputCount ) ) * randn( outputCount, hiddenCount + 1 ) ); </code></pre> <p>The multiplier is just some advice I picked up on a course, I think that it is backed by a research paper that compared a few different approaches.</p> <p>In addition, you may need a <em>lot</em> of iterations to learn XOR if you run basic gradient descent. I suggest running for at least 10000 before declaring that learning isn't working. The <code>fminunc</code> function should do better than that.</p> <p>I ran <em>your</em> code with 2 hidden neurons, basic gradient descent and the above initialisations, and it learned XOR correctly. I also tried adding momentum terms, and the learning was faster and more reliable, so I suggest you take a look at that next.</p>
<p>You need at least 3 neurons in the hidden layer and correct the initialization as the first answer suggest. If the sigmoidGradient(z2) means a2.*(1-a2) all the rest of the code seems ok to me.</p> <p>Best reggards,</p>