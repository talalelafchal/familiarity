Handling empty components in EM implementation for GMM learning
<p>I am trying to implement learning a Gaussian Mixture Model using EM from scratch in MATLAB. The project requires some later modifications to the standard GMM model, which is why I am not using off-the-shelf implementations such as VLFeat or the Stats Toolbox. Rolling out an implementation would be a learning experience and be easily customizable later on.</p> <p>Specifically, coding EM for a GMM with <strong>spherical covariances</strong>.</p> <ol> <li><p><strong>Handling empty clusters.</strong> I am having trouble handling the case when some components of the GMM are not assigned any data - they have zero or negligible posterior probability mass. This case arises when there are a large number of clusters defined. What is the standard way of handling this case?</p></li> <li><p>Intuitively, I would select the component with highest covariance and assign half of its data to the empty component.</p></li> </ol> <p>My question is: is there a standard and principled way of handling this in EM implementations (which I haven't managed to find via Google)?</p>
<p>Empty components in GMM should not arise.</p> <p>Usually, you do soft assignments, so at least a tiny fraction of some object <em>will</em> remain in every component. This is why you need a convergence threshold for EM.</p>