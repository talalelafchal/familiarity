svmtrain - Specify the cost for missclassification
<p>Matlab offers the command fitcsvm to train an svm. In it, you can pass a key-value pair called Cost, which specifies a custom miss-classification penalty for the SVM for each class. Since I'm using an older version of matlab, I need to use svmtrain. However, I cannot find such a key-value pair for the function. Is there any way I could do so?</p>
<p>The cost parameter of an C-SVM is also called "boxconstraint". Please see its usage details in <a href="http://www.mathworks.com/help/stats/svmtrain.html" rel="nofollow">this index</a>.</p> <p>An C-SVM with different cost parameters for each class is called 2C-SVM. It is important to know that this strategy is best suitable for dealing with unbalanced binany datasets, in which the number of samples in one the classes is much higher than the other. A good pair of misclassifications costs in this 2C-SVM technique can also reduce the ratio of false positives (or false negatives) on balanced datasets, but it is usually very costly to optimize those parameters. If you have time, take a look on a technique called "bias shifting". With this technique, you train an C-SVM model with a single cost parameter and after that you increase (or decrease) the bias (b) parameter to control the ratio of false positives (or false negatives). It is much faster than 2C-SVM and gives comparable results.</p>
<p>If you want to classify false positives with greater accuracy than false negatives, I think you can pass <code>C</code> as a vector. Please read <a href="http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel">this</a> and <a href="http://www.svms.org/parameters/" rel="nofollow">this</a> to understand what <code>C</code> does in SVM. I quote from the second link given:</p> <blockquote> <p>"However, it is critical here, as in any regularization scheme, that a proper value is chosen for C, the penalty factor. If it is too large, we have a high penalty for nonseparable points and we may store many support vectors and overfit. If it is too small, we may have underfitting." Alpaydin (2004), page 224.</p> </blockquote> <p>So, you may pass large values of <code>C</code> for false positives and tiny values of <code>C</code> for false negatives. You have to take care of lot of other factors, for example, overfitting and under-performing. Usually, setting a large value for <code>C</code> achieves a good performance on train but it worsens on test set due to overfitting. A (very) tiny value of <code>C</code> may ignore constraints and produce a sub-optimal classification result i.e. you may be able to get a <em>significantly</em> higher performance with a different value of <code>C</code>. To avoid this, you can do cross-validation. I have never tried this.</p>