Variable error rate of SVM Classifier using K-Fold Cross Vaidation Matlab
<p>I'm using K-Fold Cross-validation to get the error rate of a SVM Classifier. This is the code with wich I'm getting the error rate for 8-Fold Cross-validation:</p> <pre><code>data = load('Entrenamiento.txt'); group = importdata('Grupos.txt'); CP = classperf(group); N = length(group); k = 8; indices = crossvalind('KFold',N,k); single_error = zeros(1,k); for j = 1:k test = (indices==j); train = ~test; SVMModel_1 = fitcsvm(data(train,:),group(train,:),'BoxConstraint',1,'KernelFunction','linear'); classification = predict(SVMModel_1,data(test,:)); classperf(CP,classification,test); single_error(1,j) = CP.ErrorRate; end confusion_matrix = CP.CountingMatrix VP = confusion_matrix(1,1); FP = confusion_matrix(1,2); FN = confusion_matrix(2,1); VN = confusion_matrix(2,2); mean_error = mean(single_error) </code></pre> <p>However, the <code>mean_error</code> changes each time I run the script. This is due to <code>crossvalind</code>, which generates random cross-validation indices, so each time I run the script, it generates different random indices. </p> <p>What should I do to calculate the true error rate? Should I calculate the mean error rate of <code>n</code> code executions? Or what value should I use?</p>
<p>You can check <a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="nofollow">wiki</a>,</p> <blockquote> <p>In k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples.</p> </blockquote> <p>and </p> <blockquote> <p>The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation.</p> </blockquote> <p>So no worries about different error rates of randomly selecting folds.</p> <p>Of course the results will be different.</p> <p>However if your error rate is in wide range then increasing <code>k</code> would help. </p> <p>Also <code>rng</code> can be used to get fixed results.</p>