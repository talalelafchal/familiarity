Type casting in arithmetic expressions in Swift
I just came across this dangerously looking code and to my surprise it works, that is, the color isn't black. UIColor(hue: 0/360, saturation: 0/100, brightness: 97/100, alpha: 1.0) Or similarly, defining the function func takesDouble(_ x: Double) { print(x) } and calling it with takesDouble(1/10) prints 0.1. Even stranger, 1.0 * ( 1/4 ) == 0.25 So clearly Swift differs from C and several other languages here. It makes me assume that as soon as an arithmetic expression contains a floating point number anywhere in it, all sub-expressions will be cast into that floating point type, regardless of any grouping by parantheses. Or more precisely: All leaf-nodes of the expression tree will be converted to that float type. Is that correct? For calling functions I'd conclude that for a function that takes a Double this comes into effect via type inference and can be thought of as if there was an implicit 1.0 * before the argument. Is that correct? This is how I understand it now, but maybe there's a deeper level to that, so I'd appreciate feedback. Thanks!