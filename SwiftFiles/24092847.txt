Why is using a generic Int that switches between 32 and 64 bits better than explicitly using Int32 and Int64
<p>In the new Apple Swift documentation, it says: </p> <blockquote> <p>Int</p> <p>In most cases, you don’t need to pick a specific size of integer to use in your code. Swift provides an additional integer type, Int, which has the same size as the current platform’s native word size:</p> <p>On a 32-bit platform, Int is the same size as Int32. On a 64-bit platform, Int is the same size as Int64. Unless you need to work with a specific size of integer, always use Int for integer values in your code. This aids code consistency and interoperability. Even on 32-bit platforms, Int can store any value between -2,147,483,648 and 2,147,483,647, and is large enough for many integer ranges.</p> </blockquote> <p>I can understand that when using APIs that are defined with "Int", you should use them. </p> <p>But for my own code, I've always been strict about using the proper bit sized types in C using the stdint header. My thought was I'd try to reduce ambiguity. However, the folks at Apple are pretty smart, and I'm wondering if I'm missing something since this is not what they recommend.</p>
<p>This subject is not widely agreed upon.</p> <p>The advantage of using a generic type is <strong>portability</strong>. The same piece of code will compile and run independent of the platform's word size. It may also be faster, in some cases.</p> <p>The advantage of using specific types is <strong>precision</strong>. There is no room for ambiguity, and the exact capabilities of the type are known ahead of time.</p> <p>There is no hard true answer. If you stick to either side of the matter for any and all purposes, you will sooner or later find yourself making an exception.</p>
<p>It is recommended where it is beneficial. In API stuffs, platform using word-sized int makes sense because it can provide biggest numeric range at best performance.</p> <p>Anyway, in some fields (like database) explicit, and clear numeric range is more important. In that case it's better to use explicit sized integer.</p> <p>Apples also uses such int64 stuffs for CoreData and CloudKit for clear numeric limits. Both are database stuffs. If you consider Apple policy much, this might helpful. </p> <p>Anyway, Swift is more C++ like, if you need more precise numeric range control, you can define your own numeric type with custom operators. Actually some of int types in Swift are defined as a struct. </p> <p>For example, here's declaration of <code>Int16</code> in Swift module,</p> <pre><code>struct Int16 : SignedInteger { var value: Builtin.Int16 init() init(_ v: Builtin.Int16) init(_ value: Int16) static func convertFromIntegerLiteral(value: Int16) -&gt; Int16 typealias ArrayBoundType = Int16 func getArrayBoundValue() -&gt; Int16 static var max: Int16 { get } static var min: Int16 { get } } </code></pre>
<p>If you want your code to use your code on different platforms (16 bits, 32bits, 64bits), follow apple recommendation. Why? Let's take an example:</p> <p>You decide to use <code>int64</code> for your return variable <code>ret</code>. That will mainly return 0 or a negative value. It will go fine on both 64-bit device and 32-bit device, although it will take some extra instructions on the 32-bit beast. However it could start to slow down execution on a 16-bit device because the system needs four words to store your variable on such system.</p> <p>If you had chosen a bare <code>int</code>, the SDK would have made your variable 32-bit wide on 32-bit devices, 64-bit wide on 64-bit machines, 16-bit wide on 16-bit machines, etc. All ok with the rest of the system (libraries, framework, whatever you work with). So not only better within the system, but better to port on other platforms.</p> <p>But my example only stands true for <em>return</em> variables, i.e. variables that take very limited number of values...</p>